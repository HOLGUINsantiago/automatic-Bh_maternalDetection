mat[, var] <- coef_list[[var]][, "Estimate"]
}
return(mat)
}
# Extraer las matrices
A_mat <- extract_estimate_matrix(model_A)
B_mat <- extract_estimate_matrix(model_B)
# Asegurar coincidencia de filas/columnas
common_rows <- intersect(rownames(A_mat), rownames(B_mat))
common_cols <- intersect(colnames(A_mat), colnames(B_mat))
A_common <- A_mat[common_rows, common_cols, drop = FALSE]
B_common <- B_mat[common_rows, common_cols, drop = FALSE]
# Matriz de diferencias
diff_mat <- A_common - B_common
# Excluir comportamientos si se pide
if (!is.null(exclude)) {
exclude_lags <- paste0(exclude, ".l1")
diff_mat <- diff_mat[
!(rownames(diff_mat) %in% exclude_lags) & rownames(diff_mat) != "const",
!(colnames(diff_mat) %in% exclude),
drop = FALSE
]
}
# Filtrar para qgraph: variables presentes en filas y columnas
row_vars <- gsub(".l1", "", rownames(diff_mat))
col_vars <- colnames(diff_mat)
valid_vars <- intersect(row_vars, col_vars)
diff_filtered <- diff_mat[paste0(valid_vars, ".l1"), valid_vars, drop = FALSE]
print(diff_filtered)
# Graficar si se desea
if (plot) {
qgraph::qgraph(diff_filtered,
layout = "circle",
directed = TRUE,
edge.labels = TRUE,
title = title)
}
return(diff_filtered)
}
compute_BW_stats <- function(df) {
df_long <- df %>%
pivot_longer(
cols = starts_with("BW.P"),
names_to = "Jour",
values_to = "Valeur"
) %>%
filter(!is.na(Valeur)) %>%
mutate(Jour_num = as.numeric(sub("BW.P", "", Jour)))
df_stats <- df_long %>%
group_by(PupID) %>%
summarise(
BW.auc = sum(diff(Jour_num) * (head(Valeur, -1) + tail(Valeur, -1)) / 2),
lm_fit = list(lm(Valeur ~ Jour_num)),
.groups = "drop"
) %>%
mutate(
slope = map_dbl(lm_fit, ~ coef(.x)[["Jour_num"]]),
rsq = map_dbl(lm_fit, ~ summary(.x)$r.squared)
) %>%
dplyr::select(-lm_fit)
df %>% left_join(df_stats, by = "PupID")
}
juvenile_df <- compute_BW_stats(juvenile_df)
juvenile_all_completed <- compute_BW_stats(juvenile_all_completed)
juvenile_df_only <- compute_BW_stats(juvenile_df_only)
juvenile_unic_completed <- compute_BW_stats(juvenile_unic_completed)
# Liste initiale des colonnes à supprimer
cols_to_eliminate <- c(
"av.geotax.lat", "av.front.lat", "av.grasp.lat",
"exit.tube", "BW.auc", "rsq"
)
# Ajouter dynamiquement les colonnes qui commencent par BW.P et flip.lat.p
cols_to_eliminate <- c(
cols_to_eliminate,
names(juvenile_df)[startsWith(names(juvenile_df), "BW.P")],
names(juvenile_df)[startsWith(names(juvenile_df), "flip.lat.p")]
)
# Garder uniquement les colonnes qui existent vraiment dans le dataframe
cols_to_eliminate <- cols_to_eliminate[cols_to_eliminate %in% names(juvenile_df)]
# Supprimer les colonnes
juvenile_df <- juvenile_df %>% dplyr::select(-all_of(cols_to_eliminate))
# Répéter pour juvenile_all_completed
cols_to_eliminate <- c(
"av.geotax.lat", "av.front.lat", "av.grasp.lat",
"exit.tube", "BW.auc", "rsq",
names(juvenile_all_completed)[startsWith(names(juvenile_all_completed), "BW.P")],
names(juvenile_all_completed)[startsWith(names(juvenile_all_completed), "flip.lat.p")]
)
cols_to_eliminate <- cols_to_eliminate[cols_to_eliminate %in% names(juvenile_all_completed)]
juvenile_all_completed <- juvenile_all_completed %>% dplyr::select(-all_of(cols_to_eliminate))
# Répéter pour juvenile_all_completed
cols_to_eliminate <- c(
"av.geotax.lat", "av.front.lat", "av.grasp.lat",
"exit.tube", "BW.auc", "rsq",
names(juvenile_unic_completed)[startsWith(names(juvenile_unic_completed), "BW.P")],
names(juvenile_unic_completed)[startsWith(names(juvenile_unic_completed), "flip.lat.p")]
)
cols_to_eliminate <- cols_to_eliminate[cols_to_eliminate %in% names(juvenile_unic_completed)]
juvenile_unic_completed <- juvenile_unic_completed %>% dplyr::select(-all_of(cols_to_eliminate))
physical_variables <- c("d.ear.twicth",	"d.ear.c.open",	"d.eye.l.open", "BW.auc")
motor_variables <- c("flip.AUC.p7.10", "norm.av.grasp.lat", "norm.av.front.lat", "d.transition", "d.walk")
transcriptomal_variables <- c("Cortp",	"CDK9",	"DGKH",	"FKBP5",	"HSP90AA1",	"HSP90AB1",	"NFATC1",	"NR3C1",	"PTGES3",	"SGK1",	"SGK3",	"SKA2",	"STAT5A")
dam_missing_variables <- c("av.freq.p3",	"av.ep.dur.p3",	"lat.p3",	"tot.n.voc.p14",	"av.freq.p14",	"av.ep.dur.p14",	"lat.p14")
# 1. Vérifier quelles colonnes sont numériques
num_cols <- sapply(juvenile_df, is.numeric)
# 2. Fonction Shapiro sur les colonnes (brutes)
shapiro_fun <- function(x) {
x <- na.omit(x)
if (length(x) < 3 || length(x) > 5000) return(NA)
shapiro.test(x)$p.value
}
# 3. Calcul normalité des colonnes numériques
normality_df <- data.frame(
Variable = names(juvenile_df)[num_cols],
N        = sapply(juvenile_df[ , num_cols, drop = FALSE], function(x) sum(!is.na(x))),
P_value  = sapply(juvenile_df[ , num_cols, drop = FALSE], shapiro_fun)
)
normality_df$Normalité_OK <- with(normality_df, P_value > 0.05 | N > 60)
print(normality_df)
# 4. Liste des variables numériques
num_vars <- names(juvenile_df)[sapply(juvenile_df, is.numeric)]
# 5. Fonction principale d’analyse
analyze_var <- function(var_name) {
results <- list()
df_tibble <- as_tibble(juvenile_df)
# Essayer modèles paramétriques (ANOVA) et fallback si nécessaire
for (i in 1:3) {
form <- switch(i,
as.formula(paste(var_name, "~ Condition")),
as.formula(paste(var_name, "~ CageID")),
as.formula(paste(var_name, "~ Condition * CageID")),
NULL)
if (!is.null(form)) {
model <- tryCatch(lm(form, data = df_tibble), error = function(e) NULL)
if (!is.null(model)) {
shapiro_p <- tryCatch(shapiro.test(residuals(model))$p.value, error = function(e) NA)
aov_result <- tryCatch(anova(model), error = function(e) NULL)
if (!is.null(aov_result)) {
p_values <- aov_result$`Pr(>F)`
factors <- rownames(aov_result)
padj <- p.adjust(p_values, method = "bonferroni")
# Si les résidus ne sont pas normaux → Kruskal
if (!is.na(shapiro_p) && shapiro_p <= 0.05) {
if (i == 1) {
data_sub <- df_tibble %>%
dplyr::select(all_of(c(var_name, "Condition"))) %>%
filter(!is.na(.data[[var_name]]))
kw_test <- tryCatch(kruskal.test(data_sub[[var_name]] ~ data_sub$Condition), error = function(e) NULL)
if (!is.null(kw_test)) {
df_model <- data.frame(
Variable  = var_name,
Modele    = "Modèle_1",
Facteur   = "Condition",
P_value   = kw_test$p.value,
P_adj     = NA,
Test      = "Kruskal-Wallis",
Shapiro_p = shapiro_p
)
results[[length(results) + 1]] <- df_model
}
} else if (i == 2) {
data_sub <- df_tibble %>%
dplyr::select(all_of(c(var_name, "CageID"))) %>%
filter(!is.na(.data[[var_name]]), !is.na(CageID))
kw_test <- tryCatch(kruskal.test(data_sub[[var_name]] ~ data_sub$CageID), error = function(e) NULL)
if (!is.null(kw_test)) {
df_model <- data.frame(
Variable  = var_name,
Modele    = "Modèle_2",
Facteur   = "CageID",
P_value   = kw_test$p.value,
P_adj     = NA,
Test      = "Kruskal-Wallis",
Shapiro_p = shapiro_p
)
results[[length(results) + 1]] <- df_model
}
}
} else {
# Résidus OK → garder ANOVA
df_model <- data.frame(
Variable  = var_name,
Modele    = paste0("Modèle_", i),
Facteur   = factors,
P_value   = p_values,
P_adj     = padj,
Test      = "ANOVA",
Shapiro_p = shapiro_p
)
results[[length(results) + 1]] <- df_model
}
}
}
}
}
# Modèle 3 et 5 : Kruskal-Wallis CageID selon Condition (LBN/CONT)
for (cond in c("LBN", "CONT")) {
data_sub <- df_tibble %>%
dplyr::select(all_of(c(var_name, "Condition", "CageID"))) %>%
filter(!is.na(.data[[var_name]]), Condition == cond, !is.na(CageID))
if (nrow(data_sub) > 2) {
kw_test <- tryCatch(kruskal.test(data_sub[[var_name]] ~ data_sub$CageID), error = function(e) NULL)
if (!is.null(kw_test)) {
df_model <- data.frame(
Variable  = var_name,
Modele    = paste0("Modèle_", ifelse(cond == "LBN", "3", "5")),
Facteur   = paste0("CageID (", cond, ")"),
P_value   = kw_test$p.value,
P_adj     = NA,
Test      = "Kruskal-Wallis",
Shapiro_p = NA
)
results[[length(results) + 1]] <- df_model
}
}
}
results <- bind_rows(results)
# Correction intra-modèle
results <- results %>%
group_by(Modele) %>%
mutate(P_adj_model = p.adjust(P_value, method = "bonferroni")) %>%
ungroup()
# Correction globale
results$P_adj_global <- p.adjust(results$P_value, method = "bonferroni")
return(results)
}
# 6. Lancer l'analyse pour chaque variable numérique
juvenile_df <- as_tibble(juvenile_df)
results_list <- lapply(num_vars, analyze_var)
full_results <- bind_rows(results_list)
# 7. Correction finale globale (si besoin)
full_results$P_adj <- p.adjust(full_results$P_value, method = "bonferroni")
# Cage effects - normal (based on P_adj_global)
cage_impacted_normal <- full_results %>%
filter(
str_detect(Facteur, "CageID") | str_detect(Facteur, "CageID \\(CONT\\)") | str_detect(Facteur, "CageID \\(LBN\\)"),
P_adj_global < 0.05 | (is.na(P_adj) & P_value < 0.05)
) %>%
pull(Variable) %>%
unique()
# Cage effects - strict (based on P_adj)
cage_impacted_strict <- full_results %>%
filter(
str_detect(Facteur, "CageID") | str_detect(Facteur, "CageID \\(CONT\\)") | str_detect(Facteur, "CageID \\(LBN\\)"),
P_adj < 0.05 | (is.na(P_adj) & P_value < 0.05)
) %>%
pull(Variable) %>%
unique()
# Condition effects - normal (with global adjusted p)
condition_impacted_normal <- full_results %>%
filter(
str_detect(Facteur, "Condition") &
str_detect(Modele, "Modèle_1") &
(P_adj_global < 0.05 | (is.na(P_adj) & P_value < 0.05))
) %>%
pull(Variable) %>%
unique()
# Condition effects - strict (with adjusted p)
condition_impacted_strict <- full_results %>%
filter(
str_detect(Facteur, "Condition") &
str_detect(Modele, "Modèle_1") &
(P_adj < 0.05 | (is.na(P_adj) & P_value < 0.05))
) %>%
pull(Variable) %>%
unique()
fill_colors <- c("CONT" = "#a00000", "LBN" = "#1a80bb")
export_csv(summary_cage, "juvenile_analysis", "juvenile_df.csv")
walk(condition_impacted_normal, ~{
print(plot_variable_box(
df = juvenile_df,
df_condition = full_results,
var_name = .x,
fill_colors = fill_colors,
y_label = paste(.x)
))
})
analyser_clusters_pca(juvenile_df, condition_impacted_strict, 2, -1)
analyser_clusters_pca(juvenile_df, condition_impacted_strict, 2, 8)
clusters <- analyser_clusters_pca(juvenile_all_completed, condition_impacted_normal, 3, -1)
cluster_rf_analysis(juvenile_all_completed, condition_impacted_normal)
analyser_clusters_pca(juvenile_all_completed, condition_impacted_normal, 4, 6)
analyser_clusters_pca(juvenile_df, cage_impacted_strict, 2, -1)
clusters <- analyser_clusters_pca(juvenile_df, cage_impacted_strict, 2, 5)
analyser_clusters_pca(juvenile_all_completed, cage_impacted_strict, 2, -1)
analyser_clusters_pca(juvenile_all_completed, cage_impacted_strict, 2, 5)
analyser_clusters_pca(juvenile_all_completed,unique(c(cage_impacted_normal, cage_impacted_strict, condition_impacted_normal, condition_impacted_strict)) , 4, -1)
cluster_rf_analysis(juvenile_all_completed, unique(c(cage_impacted_normal, cage_impacted_strict, condition_impacted_normal, condition_impacted_strict)))
analyser_clusters_pca(juvenile_all_completed, cage_impacted_normal, 4, 6)
group_VARs <- fit_VAR_models(
df = maternal_full_df,
bin_size = 60,
auto_lag = TRUE,
lag_max = 10
)
options(warn = -1)
VAR_names <- names(group_VARs)
Cage_Group_Day_names <- VAR_names %>%
str_split("_", n = 4, simplify = TRUE) %>%
as_tibble() %>%
transmute(key = paste(V1, V3, V4, sep = "_")) %>%
pull(key)
indices_by_group <- split(seq_along(group_VARs), Cage_Group_Day_names)
group_day_P_matrices <- list()
for (group_key in names(indices_by_group)) {
indices <- indices_by_group[[group_key]]
P_matrices_list <- lapply(indices, function(idx) {
extract_P_matrix(group_VARs[[idx]])
})
if (length(P_matrices_list) == 0) next
# Filtrar matrices con las mismas dimensiones (por seguridad)
dims <- sapply(P_matrices_list, dim)
keep <- apply(dims, 2, function(x) all(x == dims[,1]))
P_matrices_list <- P_matrices_list[keep]
# Promediar elemento por elemento
P_mean <- Reduce("+", P_matrices_list) / length(P_matrices_list)
group_day_P_matrices[[group_key]] <- P_mean
}
plot_VAR_graph(group_day_P_matrices[["1_LBN_1"]], title= "1_LBN_1")
plot_VAR_graph(group_day_P_matrices[["1_LBN_2"]], title= "1_LBN_2")
#plot_VAR_graph(group_day_P_matrices[["1_LBN_3"]], title= "1_LBN_3")
plot_VAR_graph(group_day_P_matrices[["1_LBN_4"]], title= "1_LBN_4")
plot_VAR_graph(group_day_P_matrices[["1_LBN_5"]], title= "1_LBN_5")
#plot_VAR_graph(group_day_P_matrices[["1_LBN_6"]], title= "1_LBN_6")
#plot_VAR_graph(group_day_P_matrices[["1_LBN_7"]], title= "1_LBN_7")
for (name in names(group_day_P_matrices)) {
mat <- group_day_P_matrices[[name]]
# Si es un objeto matriz válido
if (!is.null(mat) && is.matrix(mat)) {
# Convertir a data.frame para exportar
df_to_export <- as.data.frame(mat)
# Guardar como CSV en "outputs/VAR_matrices"
export_csv(df_to_export, "maternal_analysis", "VAR_matrices", paste0(name, ".csv"))
}
}
options(warn = 0)
Condition_names <- names(group_day_P_matrices) %>%
str_split("_", n = 3, simplify = TRUE) %>%
as_tibble() %>%
transmute(key = V2) %>%
pull(key)
indices_by_condition <- split(seq_along(group_day_P_matrices), Condition_names)
condition_P_matrices <- list()
for (condition_key in names(indices_by_condition)) {
indices <- indices_by_condition[[condition_key]]
P_matrices_list <- lapply(indices, function(idx) {
group_day_P_matrices[[idx]]
})
if (length(P_matrices_list) == 0) next
# Filtrar matrices con mismas dimensiones (seguridad)
dims <- sapply(P_matrices_list, dim)
keep <- apply(dims, 2, function(x) all(x == dims[,1]))
P_matrices_list <- P_matrices_list[keep]
# Promedio elemento por elemento
P_mean <- Reduce("+", P_matrices_list) / length(P_matrices_list)
condition_P_matrices[[condition_key]] <- P_mean
}
# Visualización de las redes de transición promedio por condición
plot_VAR_graph(condition_P_matrices[["LBN"]], title = "LBN")
plot_VAR_graph(condition_P_matrices[["CONT"]], title = "CONT")
for (name in names(condition_P_matrices)) {
mat <- condition_P_matrices[[name]]
# Si es un objeto matriz válido
if (!is.null(mat) && is.matrix(mat)) {
# Convertir a data.frame para exportar
df_to_export <- as.data.frame(mat)
# Guardar como CSV en "outputs/VAR_matrices"
export_csv(df_to_export, "maternal_analysis", "dam_condition_day", paste0(name, ".csv"))
}
}
matrix_names <- names(group_day_P_matrices)
# Extraer el CAGE de cada nombre (ej: "1_LBN_3" -> "1")
cage_condition_ids <- matrix_names %>%
str_split("_", simplify = TRUE) %>%
apply(1, function(x) paste(x[1], x[2], sep = "_"))
indices_by_cage <- split(seq_along(group_day_P_matrices), cage_condition_ids)
cage_P_matrices <- list()
for (cage_key in names(indices_by_cage)) {
indices <- indices_by_cage[[cage_key]]
matrices_list <- lapply(indices, function(idx) group_day_P_matrices[[idx]])
# Filtrar matrices con dimensiones iguales
dims <- sapply(matrices_list, dim)
keep <- apply(dims, 2, function(x) all(x == dims[,1]))
matrices_list <- matrices_list[keep]
# Calcular promedio elemento a elemento
P_mean <- Reduce("+", matrices_list) / length(matrices_list)
cage_P_matrices[[cage_key]] <- P_mean
}
cage_keys <- names(cage_P_matrices)
n <- length(cage_keys)
frobenius_mat <- matrix(NA, nrow = n, ncol = n, dimnames = list(cage_keys, cage_keys))
for (i in seq_along(cage_keys)) {
for (j in seq_along(cage_keys)) {
if (i <= j) {
mat_i <- cage_P_matrices[[cage_keys[i]]]
mat_j <- cage_P_matrices[[cage_keys[j]]]
# Excluir variable "Carryingpups"
var_names <- rownames(mat_i)
keep_vars <- which(var_names != "Carryingpups")
mat_i_red <- mat_i[keep_vars, keep_vars, drop=FALSE]
mat_j_red <- mat_j[keep_vars, keep_vars, drop=FALSE]
# Calcular distancia de Frobenius
dist_val <- tryCatch({
diff_matrix <- mat_i_red - mat_j_red
norm(diff_matrix, type = "F")
}, error = function(e) NA)
frobenius_mat[i, j] <- dist_val
frobenius_mat[j, i] <- dist_val
}
}
}
frobenius_df <- as.data.frame(as.table(frobenius_mat)) %>%
rename(CageA = Var1, CageB = Var2, Frobenius_Dist = Freq) %>%
filter(CageA != CageB) %>%
drop_na()
frobenius_mat <- frobenius_mat[, colSums(!is.na(frobenius_mat)) > 0]
frobenius_mat <- frobenius_mat[rowSums(!is.na(frobenius_mat)) > 0, ]
df_to_export <- as.data.frame(frobenius_mat)
export_csv(df_to_export, "VAR_maternal",  "all_cages_differences.csv")
pheatmap(
frobenius_mat,
main = "Frobenius Distance Between Cages (using mean P matrices)",
display_numbers = TRUE,
clustering_method = "ward.D2",
fontsize_row = 10,
fontsize_col = 10
)
# Columnas de comportamientos y fenotipos
beh_cols <- c("Onnest_bouts", "Offnest_bouts", "ABN_bouts", "Carryingpups_bouts", "Selfgrooming_bouts", "Eat_drink_bouts", "Kicking_bouts", "Groomingpups_bouts", "Build_bouts","Onnest_duration", "Offnest_duration", "ABN_duration", "Carryingpups_duration", "Selfgrooming_duration", "Eat_drink_duration", "Kicking_duration", "Groomingpups_duration", "Build_duration")
df_maternal_summary <- maternal_df %>%
group_by(Cage.ID) %>%
filter(Group == "LBN") %>%
summarise(across(where(is.numeric), sum, na.rm = TRUE))
df_maternal_summary <- df_maternal_summary %>%
dplyr::select(Cage.ID, all_of(beh_cols))
df_fenotipos_summary <- juvenile_all_completed %>%
group_by(CageID) %>%
summarise(across(where(is.numeric), mean, na.rm = TRUE))
df_merged <- inner_join(
df_maternal_summary,
df_fenotipos_summary,
by = c("Cage.ID" = "CageID")  # ← empareja nombres diferentes
)
pup_cols <- setdiff(colnames(df_merged), c("Cage.ID", beh_cols))
behav_data <- df_merged %>% dplyr::select(all_of(beh_cols))
pup_data <- df_merged %>% dplyr::select(all_of(pup_cols))
# Eliminar columnas con varianza 0
behav_data <- behav_data %>% dplyr::select(where(~ var(.x, na.rm=TRUE) > 0))
pup_data <- pup_data %>% dplyr::select(where(~ var(.x, na.rm=TRUE) > 0))
# Correlaciones cruzadas
cor_cross <- cor(behav_data, pup_data, use = "pairwise.complete.obs")
# p-valores cruzados con doble bucle
pvals_cross <- matrix(NA, nrow = ncol(behav_data), ncol = ncol(pup_data), dimnames = list(colnames(behav_data), colnames(pup_data)))
for(i in 1:ncol(behav_data)){
for(j in 1:ncol(pup_data)){
x <- behav_data[[i]]
y <- pup_data[[j]]
if(sum(!is.na(x) & !is.na(y)) > 3){
pvals_cross[i,j] <- cor.test(x,y)$p.value
}
}
}
# Filtrar correlaciones: p < 0.05 y |r| > 0.5
cor_cross_filtered <- cor_cross
cor_cross_filtered[pvals_cross > 0.05 | abs(cor_cross) < 0.5] <- NA
# Visualizar
custom_colors <- colorRampPalette(c("blue", "yellow", "red"))(100)
# Use the custom color gradient in corrplot
corrplot(t(cor_cross_filtered),
method = "color",
col = custom_colors,
na.label = " ",
tl.cex = 0.8,
title = "",
mar = c(0, 0, 2, 0),
tl.srt = 45)
# Function to train models using only behavioral data
models <- list()
for (phenotype in colnames(pup_data)) {
formula <- as.formula(paste(phenotype, "~ ."))
print(formula)
model <- lm(formula, data = cbind(behav_data, pup_data[phenotype]))
print(model)
models[[phenotype]] <- model
}
# Summarize models
summaries <- lapply(models, summary)
# Print summaries
for (phenotype in names(summaries)) {
cat("Summary for", phenotype, ":\n")
print(summaries[[phenotype]])
cat("\n")
}
# Predict phenotypes using only behavioral data
predictions <- data.frame(Cage.ID = df_merged$Cage.ID)
for (phenotype in names(models)) {
predictions[[phenotype]] <- predict(models[[phenotype]], newdata = behav_data)
}
# Print predictions
print(predictions)
# Generate heatmap using pheatmap
predictions_heatmap_data <- predictions %>%
dplyr::select(-Cage.ID)
pheatmap(t(as.matrix(predictions_heatmap_data)),
main = "Heatmap of Predicted Phenotypes",
cluster_rows = TRUE,
cluster_cols = TRUE,
show_rownames = TRUE,
show_colnames = TRUE,
fontsize = 8,
fontsize_row = 8,
fontsize_col = 8)
